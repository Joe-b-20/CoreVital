%% Operations hierarchy: all nodes verified against cli.py, collector.py, causal_lm.py, seq2seq.py,
%% step_processor.py, baselines.py, hf_loader.py, report_builder.py, config.py, sinks
%% Updated: 2026-02-28 — audit-57 refactor. Split modules, per-request Generator, calibrate command.
%% Ref: src/CoreVital/instrumentation/{collector,causal_lm,seq2seq,step_processor,baselines}.py
%%{init: {'theme':'dark', 'themeVariables': {'primaryColor':'#1e3a5f', 'primaryTextColor':'#e2e8f0', 'lineColor':'#60a5fa'}}}%%
flowchart TB
    T[total_wall_time]

    T --> Config[config_load]
    T --> Log[setup_logging]
    T --> ML[model_load]
    T --> GenSetup["per-request torch.Generator<br/>(no global manual_seed)"]
    T --> Tok[tokenize + assert batch_size=1]
    T --> PromptCheck["prompt_length check<br/>vs max_position_embeddings"]
    T --> Inf[model_inference]
    T --> RB[report_build]
    T --> Perf[inject performance if --perf]
    T --> SW[sink_write]

    Config --> C1[from_yaml or from_default]
    Config --> C2[CLI overrides]

    ML --> M1[_resolve_device]
    ML --> M2[_resolve_dtype]
    ML --> M3[AutoTokenizer.from_pretrained]
    ML --> M4[pad_token = eos if None]
    ML --> M5[AutoConfig.from_pretrained]
    ML --> M6[ModelCapabilities.from_config]
    ML --> M6a[model_class from capabilities]
    ML --> M7[BitsAndBytesConfig if quantize]
    ML --> M8[model_class.from_pretrained]
    ML --> M9[model.to device if not quant]
    ML --> M10[model.eval]
    ML --> M11[set_attn_implementation eager if SDPA]
    ML --> M11a["_probe_attentions_available()"]
    ML --> M12[extract metadata]
    ML --> M13[_detect_quantized_dtype if quant]

    M6 --> M6b[config.is_encoder_decoder]
    M6 --> M6c[model_type lookup]
    M6 --> M6d[architecture pattern match]
    M6 --> M6e[default causal_lm]

    T --> PromptFwd[baselines.run_prompt_forward]
    PromptFwd --> PF1[model input_ids CausalLM only]
    PromptFwd --> PF2[extract prompt hidden_states]
    PromptFwd --> PF3[extract prompt attentions]
    PromptFwd --> PF4[extract prompt logits CausalLM]
    PromptFwd --> PF5[Seq2Seq: reuse encoder outputs]

    Inf --> CausalPath["causal_lm.run_causal_generation()"]
    Inf --> Seq2SeqPath["seq2seq.run_seq2seq_generation()"]

    CausalPath --> Cg["model.generate()<br/>output_hidden_states, output_attentions, output_scores"]
    CausalPath --> CStep["per-step: normalize_step_tensors + process_step"]

    Seq2SeqPath --> S1["encoder_forward .detach .cpu"]
    Seq2SeqPath --> S2[decoder_loop with KV cache]

    S2 --> S2prep["_resolve_special_token decoder_start_token_id<br/>_normalize_eos → set<br/>_build_logits_processor"]
    S2prep --> S2a[decoder_loop for step in max_new_tokens]
    S2a --> S2a1[model forward decoder_input_ids encoder_outputs past_key_values]
    S2a --> S2a2[extract next_token_logits]
    S2a --> S2a3[extract decoder_hidden_states]
    S2a --> S2a4[extract decoder_attentions]
    S2a --> S2a5[extract cross_attentions]
    S2a --> S2a6["sample via LogitsProcessorList<br/>(Temperature, TopK, TopP warpers)"]
    S2a --> S2a7[append to decoder_input_ids]
    S2a --> S2a8[check EOS break]
    S2a --> SStep["normalize_step_tensors + process_step<br/>→ StepSummary (raw tensors discarded)"]

    RB --> R1[_build_model_info]
    RB --> R2[_build_run_config]
    RB --> R3[_build_prompt_info]
    RB --> R4[_build_generated_info]
    RB --> R5[_build_timeline from List~StepSummary~]
    RB --> R6[_build_prompt_analysis]
    RB --> R7[_build_health_flags]
    RB --> R7a[_build_encoder_layers if Seq2Seq]
    RB --> R8[build Summary]
    RB --> R9[convert warnings]
    RB --> R10[assemble Report v0.4.0]
    RB --> R11[compute extensions]

    R5 --> S3[_build_layers_from_step_summary per step]

    R11 --> Ext_CS[detect_compound_signals]
    R11 --> Ext_RS["compute_risk_score (composite)<br/>+ compute_layer_blame (enriched)<br/>+ compute_layer_blame_flat"]
    R11 --> Ext_FP["compute_fingerprint_vector (25-elem v2)<br/>+ compute_prompt_hash"]
    R11 --> Ext_EW["compute_early_warning<br/>(profile threshold)"]
    R11 --> Ext_NAR["build_narrative<br/>(data-specific text)"]
    R11 --> Ext_CAL["calibration: load profile,<br/>compute_divergence_score<br/>(if configured)"]
    R11 --> Ext_MC["validate_metric_consistency<br/>(if DEBUG logging)"]
    R11 --> Ext_OR[on_risk: attach full layers if triggered]
    R11 --> Ext_RAG[RAG context if provided]

    style T fill:#1e40af,stroke:#3b82f6,color:#93c5fd
    style ML fill:#065f46,stroke:#10b981,color:#6ee7b7
    style M6 fill:#065f46,stroke:#10b981,color:#6ee7b7
    style M11a fill:#065f46,stroke:#10b981,color:#6ee7b7
    style Inf fill:#5b21b6,stroke:#8b5cf6,color:#c4b5fd
    style CausalPath fill:#5b21b6,stroke:#8b5cf6,color:#c4b5fd
    style Seq2SeqPath fill:#5b21b6,stroke:#8b5cf6,color:#c4b5fd
    style CStep fill:#14532d,stroke:#4ade80,color:#bbf7d0
    style SStep fill:#14532d,stroke:#4ade80,color:#bbf7d0
    style RB fill:#92400e,stroke:#f59e0b,color:#fde68a
    style PromptFwd fill:#7c2d12,stroke:#fb923c,color:#fed7aa
    style PF1 fill:#7c2d12,stroke:#fb923c,color:#fed7aa
    style PF2 fill:#7c2d12,stroke:#fb923c,color:#fed7aa
    style PF3 fill:#7c2d12,stroke:#fb923c,color:#fed7aa
    style PF4 fill:#7c2d12,stroke:#fb923c,color:#fed7aa
    style PF5 fill:#7c2d12,stroke:#fb923c,color:#fed7aa
    style R6 fill:#92400e,stroke:#f59e0b,color:#fde68a
    style R7 fill:#831843,stroke:#f472b6,color:#fbcfe8
    style R11 fill:#4c1d95,stroke:#a78bfa,color:#ddd6fe
    style Ext_CS fill:#4c1d95,stroke:#a78bfa,color:#ddd6fe
    style Ext_RS fill:#4c1d95,stroke:#a78bfa,color:#ddd6fe
    style Ext_FP fill:#4c1d95,stroke:#a78bfa,color:#ddd6fe
    style Ext_EW fill:#4c1d95,stroke:#a78bfa,color:#ddd6fe
    style Ext_NAR fill:#4c1d95,stroke:#a78bfa,color:#ddd6fe
    style Ext_CAL fill:#065f46,stroke:#10b981,color:#6ee7b7
    style Ext_MC fill:#1e3a5f,stroke:#60a5fa,color:#93c5fd
    style Ext_OR fill:#4c1d95,stroke:#a78bfa,color:#ddd6fe
    style Ext_RAG fill:#1e3a5f,stroke:#60a5fa,color:#93c5fd
    style GenSetup fill:#065f46,stroke:#10b981,color:#6ee7b7
    style PromptCheck fill:#065f46,stroke:#10b981,color:#6ee7b7
    style Perf fill:#0e7490,stroke:#22d3ee,color:#a5f3fc
    style SW fill:#0e7490,stroke:#22d3ee,color:#a5f3fc
