%% CoreVital Metrics Data Flow: Sources → Shared Computations → Individual Metrics → Schema Output
%% Shows what data each metric needs, where it comes from, how it's calculated, and what it produces.
%% Updated: 2026-02-28 — audit-57 refactor. Renames: voter_agreement → topk_mass, topk → topk_probs.
%% New: entropy_mean_normalized, clip diagnostics, step_processor pipeline, interpretation layer.
%% Ref: src/CoreVital/instrumentation/summaries/, step_processor.py
%%{init: {'theme':'dark', 'themeVariables': {'primaryColor':'#1e3a5f', 'primaryTextColor':'#e2e8f0', 'lineColor':'#60a5fa', 'clusterBkg':'#1e293b', 'clusterBorder':'#475569'}}}%%

flowchart TB

    %% ───────────────────────────────────────────────────────────────────
    %% RAW MODEL OUTPUTS (left-most sources)
    %% ───────────────────────────────────────────────────────────────────
    subgraph SRC["RAW MODEL OUTPUTS (from HF generate / manual decoder)"]
        direction LR
        logits["logits<br/>[batch, seq, vocab_size]<br/>Per generated step"]
        hidden["hidden_states<br/>[layers × (batch, seq, dim)]<br/>Per step (embedding stripped)"]
        attn_w["attention_weights<br/>[layers × (batch, heads, seq, seq)]<br/>Per step (last query sliced)"]
        prompt_out["prompt_outputs<br/>(extra forward pass)<br/>logits + hidden + attentions<br/>for ALL prompt tokens"]
        sampled["sampled_token_id<br/>(from generate)"]
        input_ids["input_ids<br/>(tokenized prompt)"]
    end

    %% ───────────────────────────────────────────────────────────────────
    %% STEP PROCESSOR: normalize_step_tensors()
    %% ───────────────────────────────────────────────────────────────────
    subgraph NORM["step_processor.normalize_step_tensors()"]
        direction LR
        N1["Strip embedding layer (hidden_states[1:])"]
        N2["Slice attention to last query position"]
        N3[".detach().cpu() all tensors"]
        N4["Shape assertions (3D hidden, 4D attn)"]
    end

    logits --> NORM
    hidden --> NORM
    attn_w --> NORM

    %% ───────────────────────────────────────────────────────────────────
    %% SHARED INTERMEDIATE COMPUTATIONS
    %% ───────────────────────────────────────────────────────────────────
    subgraph SHARED["SHARED INTERMEDIATES  (compute once, use many)"]
        direction LR
        log_sm["log_softmax(logits)<br/>→ log_probs"]
        sm["exp(log_probs)<br/>→ probs"]
        topk["topk(probs, k=10)<br/>→ top_k_values, top_k_indices"]
        head_ent["per_head_entropy<br/>-Σ(a · log(clamp(a, min=1e-10)))<br/>[heads] per layer<br/>+ normalize by log(K)"]
        head_conc["per_head_max<br/>max(a, dim=-1)<br/>[heads] per layer"]
    end

    NORM --> log_sm
    log_sm --> sm
    sm --> topk
    NORM --> head_ent
    NORM --> head_conc

    %% ───────────────────────────────────────────────────────────────────
    %% LOGIT METRICS
    %% ───────────────────────────────────────────────────────────────────
    subgraph LOGIT["LOGIT METRICS — summaries/logits.py"]
        direction TB
        M1["<b>Shannon Entropy</b><br/>H = -Σ(p · log_p) / log(2)<br/><i>Output:</i> entropy: float<br/><i>Mode:</i> full or topk_approx<br/><i>Signal:</i> > threshold = confusion"]
        M2["<b>Top-K Margin</b><br/>prob[0] - prob[1]<br/><i>Output:</i> top_k_margin: float<br/><i>Signal:</i> < 0.2 = model torn"]
        M3["<b>Top-K Mass (topk_mass)</b><br/>Σ(top_k_probs)<br/><i>Output:</i> topk_mass: float<br/><i>Alias:</i> voter_agreement (deprecated)"]
        M4["<b>Top-K Probs (topk_probs)</b><br/>Individual top-K probabilities<br/><i>Alias:</i> topk (deprecated)"]
    end

    log_sm --> M1
    sm --> M1
    topk --> M2
    topk --> M3
    topk --> M4

    %% ───────────────────────────────────────────────────────────────────
    %% ATTENTION METRICS (generation timeline)
    %% ───────────────────────────────────────────────────────────────────
    subgraph ATTN_GEN["ATTENTION METRICS — summaries/attention.py"]
        direction TB
        M5["<b>Attention Entropy</b><br/>mean/min/max of per_head_entropy<br/><i>Output:</i> entropy_mean, entropy_mean_normalized,<br/>entropy_min, entropy_max<br/>collapsed_head_count, focused_head_count<br/><i>Collapse:</i> normalized < 0.03"]
        M6["<b>Concentration</b><br/>max/min of per_head_max<br/><i>Output:</i> concentration_max, concentration_min<br/><i>Normalization:</i> division (not softmax)"]
    end

    head_ent --> M5
    head_conc --> M6

    %% ───────────────────────────────────────────────────────────────────
    %% PROMPT ATTENTION METRICS
    %% ───────────────────────────────────────────────────────────────────
    subgraph ATTN_PROMPT["ATTENTION METRICS — Prompt Telemetry"]
        direction TB
        M7a["<b>Sparse Extraction</b><br/>torch.where(attn > 0.01)<br/><i>Output:</i> SparseAttentionHead (SoA)"]
        M7b["<b>Basin Score (vectorized)</b><br/>avg(middle_attn) / avg(boundary_attn)<br/><i>Output:</i> basin_scores per layer<br/><i>Signal:</i> < 0.3 = Lost in the Middle"]
    end

    prompt_out --> M7a
    prompt_out --> M7b

    %% ───────────────────────────────────────────────────────────────────
    %% HIDDEN STATE METRICS
    %% ───────────────────────────────────────────────────────────────────
    subgraph HIDDEN["HIDDEN STATE METRICS — summaries/hidden_states.py"]
        direction TB
        M8["<b>L2 Norm</b><br/>‖h‖₂ = √Σ(h_i²)<br/><i>Output:</i> l2_norm: float per layer"]
        M9["<b>Standard Deviation</b><br/>std(h, dim=-1).mean()"]
        M10["<b>Layer Transformation</b><br/>1 - cos_sim(h_n, h_{n-1})"]
        M11["<b>Mean</b><br/>mean(h, dim=-1).mean()"]
        M_CLIP["<b>Clip Diagnostics</b><br/><i>Output:</i> clip_fraction, clip_max_before<br/>(when clipping fires)"]
    end

    NORM --> M8
    NORM --> M9
    NORM --> M10
    NORM --> M11
    NORM --> M_CLIP
    prompt_out --> M10

    %% ───────────────────────────────────────────────────────────────────
    %% ANOMALY DETECTION
    %% ───────────────────────────────────────────────────────────────────
    subgraph ANOMALY["ANOMALY DETECTION — Per-Layer Per-Tensor"]
        direction TB
        M12["<b>NaN/Inf Detection</b><br/>isnan(tensor).any(), isinf(tensor).any()<br/><i>Output:</i> TensorAnomalies(has_nan, has_inf)"]
    end

    NORM --> M12

    %% ───────────────────────────────────────────────────────────────────
    %% DERIVED METRICS
    %% ───────────────────────────────────────────────────────────────────
    subgraph DERIVED["DERIVED METRICS"]
        direction TB
        M13["<b>Perplexity</b><br/>2^entropy<br/><i>Invariant:</i> perplexity = 2^entropy"]
        M13_5a["<b>Surprisal (generation)</b><br/>-log₂(p_actual_token)<br/>(.detach().cpu().float(), torch.no_grad())"]
        M13_5b["<b>Surprisal (prompt)</b><br/>CrossEntropyLoss(reduction=none)"]
    end

    M1 ==>|"entropy value"| M13
    sm -->|"probs"| M13_5a
    sampled -->|"actual token"| M13_5a
    prompt_out -->|"prompt logits"| M13_5b
    input_ids -->|"shifted labels"| M13_5b

    %% ───────────────────────────────────────────────────────────────────
    %% STEP PROCESSOR OUTPUT: StepSummary
    %% ───────────────────────────────────────────────────────────────────
    subgraph STEP_OUT["step_processor.process_step() → StepSummary"]
        direction TB
        SS["<b>StepSummary</b> (scalars only)<br/>logits_summary, layer_summaries,<br/>_last_layer_hidden_vec (small 1-D CPU)<br/>Raw tensors discarded here"]
    end

    M1 --> SS
    M2 --> SS
    M3 --> SS
    M5 --> SS
    M6 --> SS
    M8 --> SS
    M9 --> SS
    M12 --> SS
    M13 --> SS
    M13_5a --> SS

    %% ───────────────────────────────────────────────────────────────────
    %% HEALTH FLAGS (aggregate from StepSummaries)
    %% ───────────────────────────────────────────────────────────────────
    subgraph HEALTH["HEALTH FLAGS — Aggregated from StepSummary timeline"]
        direction TB
        M14["<b>Health Flags</b><br/>nan/inf, attention_collapse (normalized < 0.03),<br/>high_entropy_steps (profile threshold),<br/>repetition_loop, mid_layer_anomaly"]
        M14a["<b>Repetition Loop</b><br/>cos_sim > 0.9995 for 3+ steps<br/>(optional token_id_buffer cross-check)"]
        M14b["<b>Mid-Layer Anomaly</b><br/>median early-layer baseline<br/>Targets middle ⅓ of layers"]
    end

    SS ==> M14
    SS ==>|"_last_layer_hidden_vec buffer"| M14a
    M14a ==> M14
    SS ==>|"L2 norms per layer"| M14b
    M14b ==> M14

    %% ═══════════════════════════════════════════════════════════════════
    %% STYLING
    %% ═══════════════════════════════════════════════════════════════════

    style SRC fill:#0f172a,stroke:#334155,color:#94a3b8
    style logits fill:#1e3a5f,stroke:#60a5fa,color:#93c5fd
    style hidden fill:#1e3a5f,stroke:#60a5fa,color:#93c5fd
    style attn_w fill:#1e3a5f,stroke:#60a5fa,color:#93c5fd
    style prompt_out fill:#1e3a5f,stroke:#60a5fa,color:#93c5fd
    style sampled fill:#1e3a5f,stroke:#60a5fa,color:#93c5fd
    style input_ids fill:#1e3a5f,stroke:#60a5fa,color:#93c5fd

    style NORM fill:#0f172a,stroke:#f59e0b,color:#fde68a
    style N1 fill:#92400e,stroke:#f59e0b,color:#fde68a
    style N2 fill:#92400e,stroke:#f59e0b,color:#fde68a
    style N3 fill:#92400e,stroke:#f59e0b,color:#fde68a
    style N4 fill:#92400e,stroke:#f59e0b,color:#fde68a

    style SHARED fill:#0f172a,stroke:#334155,color:#94a3b8
    style log_sm fill:#134e4a,stroke:#2dd4bf,color:#99f6e4
    style sm fill:#134e4a,stroke:#2dd4bf,color:#99f6e4
    style topk fill:#134e4a,stroke:#2dd4bf,color:#99f6e4
    style head_ent fill:#134e4a,stroke:#2dd4bf,color:#99f6e4
    style head_conc fill:#134e4a,stroke:#2dd4bf,color:#99f6e4

    style LOGIT fill:#1e1b4b15,stroke:#6366f1,color:#a5b4fc
    style M1 fill:#312e81,stroke:#818cf8,color:#c7d2fe
    style M2 fill:#312e81,stroke:#818cf8,color:#c7d2fe
    style M3 fill:#312e81,stroke:#818cf8,color:#c7d2fe
    style M4 fill:#312e81,stroke:#818cf8,color:#c7d2fe

    style ATTN_GEN fill:#7c2d1215,stroke:#f97316,color:#fed7aa
    style M5 fill:#7c2d12,stroke:#fb923c,color:#fed7aa
    style M6 fill:#7c2d12,stroke:#fb923c,color:#fed7aa

    style ATTN_PROMPT fill:#7c2d1215,stroke:#f97316,color:#fed7aa
    style M7a fill:#7c2d12,stroke:#fb923c,color:#fed7aa
    style M7b fill:#7c2d12,stroke:#fb923c,color:#fed7aa

    style HIDDEN fill:#14532d15,stroke:#22c55e,color:#bbf7d0
    style M8 fill:#14532d,stroke:#4ade80,color:#bbf7d0
    style M9 fill:#14532d,stroke:#4ade80,color:#bbf7d0
    style M10 fill:#14532d,stroke:#4ade80,color:#bbf7d0
    style M11 fill:#14532d,stroke:#4ade80,color:#bbf7d0
    style M_CLIP fill:#14532d,stroke:#4ade80,color:#bbf7d0

    style ANOMALY fill:#7f1d1d15,stroke:#ef4444,color:#fecaca
    style M12 fill:#7f1d1d,stroke:#f87171,color:#fecaca

    style DERIVED fill:#713f1215,stroke:#eab308,color:#fef08a
    style M13 fill:#713f12,stroke:#facc15,color:#fef9c3
    style M13_5a fill:#713f12,stroke:#facc15,color:#fef9c3
    style M13_5b fill:#713f12,stroke:#facc15,color:#fef9c3

    style STEP_OUT fill:#92400e15,stroke:#f59e0b
    style SS fill:#92400e,stroke:#f59e0b,color:#fde68a

    style HEALTH fill:#4c002515,stroke:#ec4899,color:#fbcfe8
    style M14 fill:#831843,stroke:#f472b6,color:#fbcfe8
    style M14a fill:#831843,stroke:#f472b6,color:#fbcfe8
    style M14b fill:#831843,stroke:#f472b6,color:#fbcfe8
