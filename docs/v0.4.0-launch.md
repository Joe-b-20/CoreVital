# CoreVital v0.4.0: Technical Launch

**CoreVital** is an open-source toolkit that monitors LLM inference from the inside: it hooks into the forward pass of Hugging Face transformers to capture hidden states, attention patterns, and logits at every generation step, then turns them into a structured report with a **risk score**, **health flags**, and **per-layer diagnostics** — without storing raw tensors.

This post summarizes what v0.4.0 delivers and how to use it.

---

## Why CoreVital?

Standard observability (tracing prompts, responses, latency) tells you *that* something went wrong, not *where inside the model*. Repetition, collapse, or subtle hallucinations often show up first in internal activations: entropy spikes, attention collapse, or hidden-state repetition. CoreVital instruments the forward pass, computes lightweight summary statistics (mean, std, L2 norm, entropy, etc.), and discards the raw tensors. You get a fixed-schema report (JSON/SQLite/Datadog/Prometheus/W&B) with:

- A **0–1 risk score** and boolean **health flags** (NaN/Inf, repetition loop, mid-layer anomaly, attention collapse, high entropy)
- **Per-step timeline**: entropy, perplexity, surprisal, top-k margin
- **Per-layer summaries** (optional): hidden-state stats, attention head collapse/focus
- **Prompt telemetry**: how the model processed the input (sparse attention, basin scores, surprisal)
- **Layer blame** and **narrative** for quick triage

No model code changes: same `generate()`-style API, with instrumentation toggled per run.

---

## What’s in v0.4.0?

- **Schema 0.4.0** — Validated report format with migration path from 0.3.0; `corevital migrate` for legacy traces.
- **Backend abstraction** — Pluggable inference backends. **HuggingFaceBackend** (default) supports full instrumentation; stubs for **vLLM**, **llama.cpp**, and **TGI** are in place for future implementations.
- **Beam search** — CausalLM supports `--num_beams` and `--early_stopping`; timeline and metrics are reported for the best beam.
- **Real-time intervention (Seq2Seq)** — Optional `step_callback` in the manual decoder loop: you can stop generation when a condition is met (e.g. repetition detected via `detect_repetition_loop()` on a rolling hidden-state buffer).
- **Risk calibration and GPU benchmarks** — [Risk calibration](risk-calibration.md) documents heuristic thresholds and planned ECE/benchmark validation. [GPU benchmarks](gpu-benchmarks.md) describes how to measure overhead with `--perf strict` and where to find results in the report.
- **Model compatibility and metrics guides** — [Model compatibility](model-compatibility.md) covers architectures, attention capture, and per-model threshold profiles. [Metrics interpretation](metrics-interpretation.md) explains how to read entropy, perplexity, surprisal, and attention metrics.
- **Dashboard** — Streamlit app: timeline charts, attention heatmaps, prompt analysis, Compare view, performance breakdown. A [hosted demo](https://corevital-dwwtkbigm89mp5opxioez3.streamlit.app/) is available.

Tested with Llama 3, Mistral, Mixtral, Qwen2, GPT-2, and Flan-T5; see the [model compatibility](model-compatibility.md) doc and `tests/test_models_production.py` (run with `pytest -m slow`).

---

## How it works (high level)

1. **Instrument** — Run generation with PyTorch hooks (or the manual Seq2Seq loop) so that hidden states, attentions, and logits are captured per step.
2. **Summarize** — Compute scalar/layer summaries in-memory; raw tensors are not persisted.
3. **Report** — Build a single report: risk score, health flags, timeline, prompt analysis, layer blame, fingerprint, narrative.
4. **Sink** — Write to SQLite (default), JSON, Datadog, Prometheus, W&B, or OpenTelemetry.

Overhead depends on capture mode: full per-layer data scales as O(steps × layers × heads). Use `--capture summary` for production (minimal payload, <5% overhead) or `--capture on_risk` to record a full trace only when risk exceeds a threshold.

---

## Try it

```bash
pip install "git+https://github.com/Joe-b-20/CoreVital.git"
corevital run --model gpt2 --prompt "Explain why the sky is blue" --max_new_tokens 20
```

Optional: `pip install "CoreVital[dashboard]"` and run `streamlit run dashboard.py` for the local dashboard, or use the [hosted demo](https://corevital-dwwtkbigm89mp5opxioez3.streamlit.app/).

**Library API:** `CoreVitalMonitor` lets you run instrumented generation and then call `get_risk_score()`, `get_health_flags()`, `get_summary()`, or `should_intervene()`. See [Integration examples](integration-examples.md) and [Production deployment](production-deployment.md).

---

## Docs and roadmap

- **README** — [Features](README.md#features), [CLI](README.md#cli-options-run), [Library API](README.md#library-api-corevitalmonitor), [Roadmap](README.md#roadmap)
- **Risk and calibration** — [risk-calibration.md](risk-calibration.md)
- **Overhead and GPU** — [gpu-benchmarks.md](gpu-benchmarks.md)
- **Design and phases** — [design-journey.md](design-journey.md)

CoreVital is **Apache-2.0** and in active development. Contributions and feedback are welcome on [GitHub](https://github.com/Joe-b-20/CoreVital).
